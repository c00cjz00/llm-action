


- 預訓練語言模型：GLM: https://zhuanlan.zhihu.com/p/641499380




GLM-130B 對超過 4000 億個雙語標記（2000 億英文和 2000 億中文標記）進行了預訓練。

它的預訓練目標由兩部分組成：

第一部分（95%）是自監督的預訓練，即在公開的大規模語料庫以及其他一些較小的中文語料庫上的自迴歸空白填充。

第二部分（5%）是在 T0++18 和 DeepStruct19 中 70 個不同資料集的抽樣子集上進行多工指令預訓練，格式為基於指令的多工多提示序列到序列的生成。

這種設計使 GLM-130B 可以在其他資料集上進行了零樣本學習，以及從英文到中文的零樣本遷移。



