



吞吐量  


延遲





投機取樣：
- https://github.com/feifeibear/LLMSpeculativeSampling

美杜莎：
- https://github.com/FasterDecoding/Medusa
- Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads
- https://sites.google.com/view/medusa-llm




- OpenLLM: https://github.com/bentoml/OpenLLM

## Triton





## 部落格

- https://huggingface.co/blog/optimize-llm
- 加速大模型推理的7種方法：https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47
- 7個大模型推理服務化框架：https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407







